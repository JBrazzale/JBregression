{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ccaa64",
   "metadata": {},
   "source": [
    "## Web Scrape (Scrapy) \n",
    "\n",
    "I decided to use scrapy for this project as I have previous experience with it. Scrapy is a great tool because it gives us a framework to build around for our scrape. It starts by creating a new file and assigning each step of the scrape to its own python file. I have found that scrapy works best when executed in the command line. I built \n",
    "the scrape in (Atom) so I will be adding the key parts of my build in this notebook. \n",
    "\n",
    "The core parts of a scrapy build are:\n",
    "- The items folder\n",
    "- the piplines folder\n",
    "- A couple of small tweaks in settings\n",
    "- building our spider for crawling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3968bbe",
   "metadata": {},
   "source": [
    "## Items\n",
    "\n",
    "This is where we designate which items we want to scrape from the website. We build out our own Scrapy class and \n",
    "pick out features to target. I have added them in the order they appear on Boxofficemojo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe386d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BoxofficeItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    domestic_revenue = scrapy.Field()\n",
    "    world_revenue = scrapy.Field()\n",
    "    distributor = scrapy.Field()\n",
    "    opening_weekend_revenue = scrapy.Field()\n",
    "    no_opening_theaters = scrapy.Field()\n",
    "    budget = scrapy.Field()\n",
    "    Release_date = scrapy.Field()\n",
    "    MPAA_rating = scrapy.Field()\n",
    "    run_time = scrapy.Field()\n",
    "    genres = scrapy.Field()\n",
    "    days_in_release = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd144b6d",
   "metadata": {},
   "source": [
    "## Spider\n",
    "\n",
    "This is where we define how we want our website to be scraped. A spider also allows us to \"crawl\" between our designated links, and how to extract our desired data from the page. You can use Beautiful soup to parse, but I decided to use lxml\n",
    "\n",
    "Here is how the scraping cycle works in my spider:\n",
    "    1. Generate an inital request to crawl from our starting URl\n",
    "        -I am targeting the top domestic box office pages, this makes things pretty easy because \n",
    "        all we have to do is change the year in our URL to get to the next page. I accomplished this \n",
    "        with a simple for loop. \n",
    "    2. We then outline callback functions to parse our selected items and return item objects\n",
    "        -this is where we can choose which selector library to use i.e BeautifulSoup or lxml\n",
    "    3. These items are sent through our pipeline and into our csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    " import scrapy\n",
    "\n",
    "from boxofficeinfo.items import BoxofficeItem\n",
    "\n",
    "class BoxofficeSpider(scrapy.Spider):\n",
    "    name = \"Boxofficeinfo\"\n",
    "    allowed_domains = [\"boxofficemojo.com\"]\n",
    "    start_urls = [\n",
    "    \"https://www.boxofficemojo.com/year/2010/\"\n",
    "    ]\n",
    "\n",
    "    for year in [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]: #desired years\n",
    "        start_urls.append(\"https://www.boxofficemojo.com/year/\"+str(year)+\"/\") #for loop to move to the next page\n",
    "\n",
    "    def parse(self, response): #outline which area of the site I want to parse from, all of my features are in one table\n",
    "        for tr in response.xpath('//*[@id=\"table\"]/div/table/tr')[1:]:\n",
    "            href = tr.xpath('./td[2]/a/@href')\n",
    "            url = response.urljoin(href[0].extract())\n",
    "            yield scrapy.Request(url, callback=self.parse_page_contents)\n",
    "\n",
    "    def parse_page_contents(self, response): #these 3 values are in their own separate section, they require a different xpath\n",
    "        item = BoxofficeItem()\n",
    "        item[\"title\"] = response.xpath('//*[@id=\"a-page\"]/main/div/div[1]/div[1]/div/div/div[2]/h1/text()')[0].extract()\n",
    "        item[\"domestic_revenue\"] = response.xpath('//*[@id=\"a-page\"]/main/div/div[3]/div[1]/div/div[1]/span[2]/span/text()')[0].extract()\n",
    "        item[\"world_revenue\"] = response.xpath('//*[@id=\"a-page\"]/main/div/div[3]/div[1]/div/div[3]/span[2]/a/span/text()')[0].extract()\n",
    "\n",
    "        elements = [] \n",
    "        for div in response.xpath('//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div')[0:]:\n",
    "            elements.append(' '.join(div.xpath('./span[1]/text()')[0].extract().split()))\n",
    "        '''\n",
    "        To get the xpath, we have to use the page inspect feature on our desired page, we can then use \n",
    "        the \"copy xpath\" on our targeted part of the page to get the correct xpath. This took me a few \n",
    "        tries, you can use scrapy shell and the url with the xpath in the command line to test for the \n",
    "        correct location. \n",
    "        some pages have missing information, or have it in different palces. This elements list that I will \n",
    "        append to helps us grab the correct information from our site. I have outlined if-else statements \n",
    "        for every element so that our spider won't stop running if a value is missing. Opening revenue also \n",
    "        the number of opening theaters attached, so I added two statements to keep them separate\n",
    "        '''\n",
    "        #Distributor\n",
    "        if 'Distributor' in elements:\n",
    "            d = elements.index('Distributor') + 1\n",
    "            loc_dist = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/text()'.format(d)\n",
    "            item[\"distributor\"] = response.xpath(loc_dist)[0].extract()\n",
    "        else:\n",
    "            item[\"distributor\"] = \"N/A\"\n",
    "\n",
    "        # Opening Revenue\n",
    "        if 'Opening' in elements:\n",
    "           o = elements.index('Opening') + 1\n",
    "           loc_open_rev = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/span/text()'.format(o)\n",
    "           try:\n",
    "               item[\"opening_revenue\"] = response.xpath(loc_open_rev)[0].extract()\n",
    "           except:\n",
    "               item[\"opening_revenue\"] = \"N/A\"\n",
    "        else:\n",
    "            item[\"opening_revenue\"] = \"N/A\"\n",
    "\n",
    "        # Opening Theaters\n",
    "        if 'Opening' in elements:\n",
    "           o = elements.index('Opening') + 1\n",
    "           loc_open_theater = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/text()'.format(o)\n",
    "           try:\n",
    "               item[\"opening_theaters\"] = response.xpath(loc_open_theater)[0].extract().split()[0]\n",
    "           except:\n",
    "               item[\"opening_theaters\"] = \"N/A\"\n",
    "        else:\n",
    "            item[\"opening_theaters\"] = \"N/A\"\n",
    "\n",
    "        # Budget\n",
    "        if 'Budget' in elements:\n",
    "            b = elements.index('Budget') + 1\n",
    "            loc_budget = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/span/text()'.format(b)\n",
    "            item[\"budget\"] = response.xpath(loc_budget)[0].extract()\n",
    "        else:\n",
    "            item[\"budget\"] = \"N/A\"\n",
    "            \n",
    "        #Release Date (already had r below, used s for season)\n",
    "        if 'Release Date' in elements: \n",
    "            s = elements.index('Release Date') + 1\n",
    "            loc_release = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/text()'.format(s)\n",
    "            item[\"Release_date\"] = response.xpath(loc_release)[0].extract() #just want the first value here\n",
    "        else:\n",
    "            item[\"Release_date\"] = \"N/A\"\n",
    "\n",
    "        # MPAA\n",
    "        if 'MPAA' in elements:\n",
    "            m = elements.index('MPAA') + 1\n",
    "            loc_MPAA = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/text()'.format(m)\n",
    "            item[\"MPAA_rating\"] = response.xpath(loc_MPAA)[0].extract()\n",
    "        else:\n",
    "            item[\"MPAA_rating\"] = \"N/A\"\n",
    "            \n",
    "        #Run Time\n",
    "        if 'Run Time' in elements:\n",
    "            t = elements.index('Run Time') + 1\n",
    "            loc_run_time = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/text()'.format(t)\n",
    "            item[\"run_time\"] = response.xpath(loc_run_time)[0].extract()\n",
    "        else:\n",
    "            item[\"run_time\"] = \"N/A\"\n",
    "\n",
    "        # Genres\n",
    "        if 'Genres' in elements:\n",
    "            g = elements.index('Genres') + 1\n",
    "            loc_genres = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/text()'.format(g)\n",
    "            item[\"genres\"] = \",\".join(response.xpath(loc_genres)[0].extract().split())\n",
    "        else:\n",
    "            item[\"genres\"] = \"N/A\"\n",
    "\n",
    "        # In Release\n",
    "        if 'In Release' in elements:\n",
    "            r = elements.index('In Release') + 1\n",
    "            loc_release = '//*[@id=\"a-page\"]/main/div/div[3]/div[4]/div[{}]/span[2]/text()'.format(r)\n",
    "            item[\"release_days\"] = response.xpath(loc_release)[0].extract().split()[0]\n",
    "        else:\n",
    "            item[\"release_days\"] = \"N/A\"\n",
    "        yield item   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3eae5",
   "metadata": {},
   "source": [
    "## Item Pipeline\n",
    "\n",
    "After our item has been scraped by a spider it gets sent to the item pipeline which processes it through \n",
    "our outlined components that are executed sequentially. Each component is a python class that implements a \n",
    "simple method. Recieve the item, perform our action (in this case write to csv), and decide if the item \n",
    "should be dropped and no longer processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98029ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class BoxofficePipeline(object): # Make sure to change to this name in the settings folder\n",
    "\n",
    "    def __init__(self): # here is my action to be performed. Write to csv with my selected row names\n",
    "        self.csvwriter = csv.writer(open(\"boxoffice_date.csv\", \"w\", newline=''))\n",
    "        self.csvwriter.writerow([\"Title\", \"Domestic_Revenue\", \"World_Revenue\", \"Distributor\", \"Opening_Weekend_Revenue\", \"no_Opening_Theaters\", \"Budget\", \"Release Date\", \"MPAA_Rating\", \"Genre\", \"Days_In_Release\" ])\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        row = []\n",
    "        row.append(item[\"Title\"])\n",
    "        row.append(item[\"Domestic_Revenue\"])\n",
    "        row.append(item[\"World_Revenue\"])\n",
    "        row.append(item[\"Distributor\"])\n",
    "        row.append(item[\"Opening_Weekend_Revenue\"])\n",
    "        row.append(item[\"no_Opening_Theaters\"])\n",
    "        row.append(item[\"Budget\"])\n",
    "        row.append(item[\"Release Date\"])\n",
    "        row.append(item[\"MPAA_Rating\"])\n",
    "        row.append(item[\"run_time\"])\n",
    "        row.append(item[\"Genre\"])\n",
    "        row.append(item[\"Days_In_Release\"])\n",
    "        self.csvwriter.writerow(row)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b84dd",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The final step with scrapy is to check and make sure the correct settings are applied. The great thing about scrapy is that it only contains settings that are considered to be important or commonly used. It also has the \n",
    "majority of its settings disabled by default.\n",
    "\n",
    "All we have to do is remove the # by the settings we want to adjust. For this scrape I only needed to change a couple of settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed069703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#this is where I added my pipeline name that we created in the spider\n",
    "ITEM_PIPELINES = {\n",
    "    'boxofficeinfo.pipelines.BoxofficePipeline': 300,\n",
    "}\n",
    "#naming our bot. It is good practice to identify ourself to the site we are scraping\n",
    "BOT_NAME = 'boxofficeinfo'\n",
    "\n",
    "SPIDER_MODULES = ['boxofficeinfo.spiders']\n",
    "NEWSPIDER_MODULE = 'boxofficeinfo.spiders'\n",
    "\n",
    "#this will obey all of the rules from the sites robot.txt \n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3fe5c",
   "metadata": {},
   "source": [
    "After customizing these pages, we can run the spider in the command line. \n",
    "\n",
    "Scrapy Documentation:\n",
    "https://docs.scrapy.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba6e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
